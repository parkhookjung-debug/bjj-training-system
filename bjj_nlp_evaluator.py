# bjj_nlp_evaluator.py
"""
BJJ V2 NLP ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì „ìš© ëª¨ë“ˆ
ì‚¬ìš©ë²•: python bjj_nlp_evaluator.py
"""

import sys
import time
import statistics
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import pandas as pd

# ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ì„ import
try:
    from bjj_advanced_system_v2 import (
        EnhancedNLPProcessor,
        ImprovedBJJDatabase,
        BJJ_BELTS
    )
except ImportError:
    print("ì˜¤ë¥˜: bjj_advanced_system_v2.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ì´ íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

class BJJNLPEvaluator:
    """BJJ NLP ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í‰ê°€ê¸° ì´ˆê¸°í™”"""
        self.nlp = EnhancedNLPProcessor()
        self.test_cases = self._load_test_cases()
        self.results = []
        
    def _load_test_cases(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜"""
        return [
            {
                "id": "weakness_beginner",
                "text": "í•˜í”„ê°€ë“œì—ì„œ ìê¾¸ ë‹¹í•˜ëŠ”ë°, ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ëª»í•˜ê² ì–´ìš”",
                "expected": {
                    "intent": "improve_weakness",
                    "difficulty_preference": "easy",
                    "level": "beginner",
                    "positions": ["guard"],
                    "emotional_state": ["frustration"],
                    "specific_techniques": ["í•˜í”„ê°€ë“œ"]
                },
                "priority": "high"
            },
            {
                "id": "negation_learning",
                "text": "íŠ¸ë¼ì´ì•µê¸€ì€ ë§ê³  ë‹¤ë¥¸ ì„œë¸Œë¯¸ì…˜ë“¤ì„ ì°¨ê·¼ì°¨ê·¼ ë°°ìš°ê³  ì‹¶ì–´ìš”",
                "expected": {
                    "intent": "learn",
                    "difficulty_preference": "normal",
                    "negation_analysis": {"has_negation": True},
                    "positions": ["submission"]
                },
                "priority": "high"
            },
            {
                "id": "competition_prep",
                "text": "ê²½ê¸° ì¤€ë¹„ ì¤‘ì¸ë° ê³µê²©ì ì¸ ê°€ë“œ íŒ¨ìŠ¤ë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ì—°ìŠµí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
                "expected": {
                    "intent": "compete",
                    "difficulty_preference": "challenging",
                    "positions": ["guard_pass"],
                    "intensity_analysis": {"level": "high"}
                },
                "priority": "high"
            },
            {
                "id": "safety_concern",
                "text": "ë¬´ë¦ ë¶€ìƒì´ ìˆì–´ì„œ ì•ˆì „í•œ ê¸°ìˆ  ìœ„ì£¼ë¡œ 30ë¶„ë§Œ ê°€ë³ê²Œ í•˜ê³  ì‹¶ì–´ìš”",
                "expected": {
                    "intent": "practice",
                    "duration": "short",
                    "safety_priority": "high",
                    "concerns_or_limitations": "ë¬´ë¦"
                },
                "priority": "high"
            },
            {
                "id": "mastery_focused",
                "text": "ë”¥í•˜í”„ê°€ë“œì—ì„œ ì™„ì „íˆ ë§ˆìŠ¤í„°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
                "expected": {
                    "intent": "strengthen",
                    "specific_techniques": ["ë”¥í•˜í”„ê°€ë“œ"],
                    "intensity_analysis": {"level": "very_high"}
                },
                "priority": "medium"
            },
            {
                "id": "synonym_test",
                "text": "í•˜í”„ê°€ë“œì—ì„œ ì•„ë¬´ë°”ë¥¼ ë°°ìš°ê³  ì‹¶ì–´ìš”",
                "expected": {
                    "intent": "learn",
                    "specific_techniques": ["í•˜í”„ê°€ë“œ", "ì•„ë¬´ë°”"],
                    "synonym_matches": {"í•˜í”„ê°€ë“œ": True, "ì•„ë¬´ë°”": True}
                },
                "priority": "medium"
            },
            {
                "id": "complex_negation",
                "text": "ìŠ¤íŒŒì´ë” ê°€ë“œ ë¹¼ê³  ë‹¤ë¥¸ ì˜¤í”ˆ ê°€ë“œë“¤ì„ ë°°ì›Œë³´ê³  ì‹¶ì–´ìš”",
                "expected": {
                    "intent": "learn",
                    "negation_analysis": {"has_negation": True},
                    "positions": ["guard"]
                },
                "priority": "medium"
            },
            {
                "id": "emotional_intensity",
                "text": "ì •ë§ ë„ˆë¬´ë„ˆë¬´ ë‹µë‹µí•´ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì‚´ì§ ë°°ì›Œë³´ê³  ì‹¶ì–´ìš”",
                "expected": {
                    "intent": "learn",
                    "emotional_state": ["frustration"],
                    "intensity_analysis": {"level": "very_high"}
                },
                "priority": "low"
            }
        ]
    
    def evaluate_single_case(self, test_case: Dict) -> Dict:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€"""
        text = test_case["text"]
        expected = test_case["expected"]
        test_id = test_case["id"]
        
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘: {test_id}")
        print(f"ì…ë ¥: {text}")
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = self.nlp.analyze_user_request(text, f"eval_user_{test_id}")
        analysis_time = time.time() - start_time
        
        # í‰ê°€ ê²°ê³¼ ê³„ì‚°
        scores = self._calculate_scores(result, expected)
        
        # ê²°ê³¼ êµ¬ì„±
        evaluation = {
            "test_id": test_id,
            "text": text,
            "analysis_time": analysis_time,
            "confidence": result.get('confidence_score', 0),
            "scores": scores,
            "result": result,
            "expected": expected,
            "priority": test_case.get("priority", "medium")
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_case_result(evaluation)
        
        return evaluation
    
    def _calculate_scores(self, result: Dict, expected: Dict) -> Dict:
        """ê°œë³„ í•­ëª©ë³„ ì ìˆ˜ ê³„ì‚°"""
        scores = {}
        
        # ì˜ë„ ë¶„ì„
        scores['intent'] = 1.0 if result.get('intent') == expected.get('intent') else 0.0
        
        # ë‚œì´ë„ ì„ í˜¸ë„
        scores['difficulty'] = 1.0 if result.get('difficulty_preference') == expected.get('difficulty_preference') else 0.0
        
        # ë ˆë²¨ ë¶„ì„
        scores['level'] = 1.0 if result.get('level') == expected.get('level') else 0.0
        
        # í¬ì§€ì…˜ ë§¤ì¹­ (ë¶€ë¶„ ì ìˆ˜ í—ˆìš©)
        result_positions = set(result.get('positions', []))
        expected_positions = set(expected.get('positions', []))
        if not expected_positions:
            scores['positions'] = 1.0  # ê¸°ëŒ€ê°’ì´ ì—†ìœ¼ë©´ ì •í™•í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
        elif result_positions & expected_positions:
            scores['positions'] = len(result_positions & expected_positions) / len(expected_positions)
        else:
            scores['positions'] = 0.0
        
        # íŠ¹ì • ê¸°ìˆ  ì¶”ì¶œ
        result_techniques = set(result.get('specific_techniques', []))
        expected_techniques = set(expected.get('specific_techniques', []))
        if not expected_techniques:
            scores['techniques'] = 1.0
        elif result_techniques & expected_techniques:
            scores['techniques'] = len(result_techniques & expected_techniques) / len(expected_techniques)
        else:
            scores['techniques'] = 0.0
        
        # V2 ê³ ê¸‰ ê¸°ëŠ¥ë“¤
        # ë¶€ì •ë¬¸ ì²˜ë¦¬
        if 'negation_analysis' in expected:
            result_negation = result.get('negation_analysis', {}).get('has_negation', False)
            expected_negation = expected['negation_analysis'].get('has_negation', False)
            scores['negation'] = 1.0 if result_negation == expected_negation else 0.0
        else:
            scores['negation'] = 1.0  # ë¶€ì •ë¬¸ì´ ê¸°ëŒ€ë˜ì§€ ì•ŠëŠ” ê²½ìš°
        
        # ê°•ë„ ë¶„ì„
        if 'intensity_analysis' in expected:
            result_intensity = result.get('intensity_analysis', {}).get('level', 'medium')
            expected_intensity = expected['intensity_analysis'].get('level', 'medium')
            scores['intensity'] = 1.0 if result_intensity == expected_intensity else 0.0
        else:
            scores['intensity'] = 1.0
        
        # ê°ì • ìƒíƒœ (ë¶€ë¶„ ì ìˆ˜)
        if 'emotional_state' in expected:
            result_emotions = set(result.get('emotional_state', []))
            expected_emotions = set(expected['emotional_state'])
            if result_emotions & expected_emotions:
                scores['emotions'] = len(result_emotions & expected_emotions) / len(expected_emotions)
            else:
                scores['emotions'] = 0.0
        else:
            scores['emotions'] = 1.0
        
        # ì•ˆì „ ê³ ë ¤ì‚¬í•­
        if 'safety_priority' in expected:
            scores['safety'] = 1.0 if result.get('safety_priority') == expected['safety_priority'] else 0.0
        else:
            scores['safety'] = 1.0
        
        # ì§€ì†ì‹œê°„
        if 'duration' in expected:
            scores['duration'] = 1.0 if result.get('duration') == expected['duration'] else 0.0
        else:
            scores['duration'] = 1.0
        
        return scores
    
    def _print_case_result(self, evaluation: Dict):
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        scores = evaluation['scores']
        test_id = evaluation['test_id']
        
        print(f"   ë¶„ì„ ì‹œê°„: {evaluation['analysis_time']:.3f}ì´ˆ")
        print(f"   ì‹ ë¢°ë„: {evaluation['confidence']:.1%}")
        
        # ì ìˆ˜ë³„ í‘œì‹œ
        for metric, score in scores.items():
            status = "âœ…" if score >= 0.8 else "âš ï¸" if score >= 0.5 else "âŒ"
            print(f"   {status} {metric}: {score:.1%}")
        
        overall = sum(scores.values()) / len(scores)
        print(f"   ğŸ“Š ì¢…í•© ì ìˆ˜: {overall:.1%}")
        print()
    
    def run_full_evaluation(self) -> Dict:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("ğŸ¤– BJJ V2 NLP ì‹œìŠ¤í…œ ì¢…í•© ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for test_case in self.test_cases:
            evaluation = self.evaluate_single_case(test_case)
            self.results.append(evaluation)
        
        # ì¢…í•© ê²°ê³¼ ê³„ì‚°
        return self._generate_summary_report()
    
    def _generate_summary_report(self) -> Dict:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ì¢…í•© ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
        all_scores = {}
        for result in self.results:
            for metric, score in result['scores'].items():
                if metric not in all_scores:
                    all_scores[metric] = []
                all_scores[metric].append(score)
        
        metric_averages = {metric: statistics.mean(scores) for metric, scores in all_scores.items()}
        
        # ì •í™•ë„ ë©”íŠ¸ë¦­ ì¶œë ¥
        print("ğŸ¯ ì •í™•ë„ ë©”íŠ¸ë¦­:")
        for metric, avg_score in metric_averages.items():
            status = "âœ…" if avg_score >= 0.8 else "âš ï¸" if avg_score >= 0.6 else "âŒ"
            print(f"   {status} {metric.capitalize()}: {avg_score:.1%}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        analysis_times = [r['analysis_time'] for r in self.results]
        confidences = [r['confidence'] for r in self.results]
        
        print(f"\nâš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   â€¢ í‰ê·  ë¶„ì„ ì‹œê°„: {statistics.mean(analysis_times):.3f}ì´ˆ")
        print(f"   â€¢ ìµœëŒ€ ë¶„ì„ ì‹œê°„: {max(analysis_times):.3f}ì´ˆ")
        print(f"   â€¢ ìµœì†Œ ë¶„ì„ ì‹œê°„: {min(analysis_times):.3f}ì´ˆ")
        print(f"   â€¢ í‰ê·  ì‹ ë¢°ë„: {statistics.mean(confidences):.1%}")
        
        # ìš°ì„ ìˆœìœ„ë³„ ì„±ëŠ¥
        priority_scores = {}
        for result in self.results:
            priority = result['priority']
            if priority not in priority_scores:
                priority_scores[priority] = []
            
            overall_score = sum(result['scores'].values()) / len(result['scores'])
            priority_scores[priority].append(overall_score)
        
        print(f"\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ì„±ëŠ¥:")
        for priority, scores in priority_scores.items():
            avg_score = statistics.mean(scores)
            print(f"   â€¢ {priority.capitalize()} priority: {avg_score:.1%} ({len(scores)}ê°œ í…ŒìŠ¤íŠ¸)")
        
        # ì „ì²´ ì¢…í•© ì ìˆ˜
        overall_accuracy = statistics.mean([
            sum(result['scores'].values()) / len(result['scores']) 
            for result in self.results
        ])
        
        print(f"\nğŸ† V2 ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {overall_accuracy:.1%}")
        
        # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
        if overall_accuracy >= 0.9:
            grade = "A+"
            comment = "íƒì›”í•œ ì„±ëŠ¥! ìƒìš© ìˆ˜ì¤€ì˜ ì •í™•ë„ì…ë‹ˆë‹¤."
        elif overall_accuracy >= 0.8:
            grade = "A"
            comment = "ìš°ìˆ˜í•œ ì„±ëŠ¥! ì‹¤ìš©ì ì¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤."
        elif overall_accuracy >= 0.7:
            grade = "B"
            comment = "ì–‘í˜¸í•œ ì„±ëŠ¥! ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif overall_accuracy >= 0.6:
            grade = "C"
            comment = "ë³´í†µ ì„±ëŠ¥. ì•Œê³ ë¦¬ì¦˜ íŠœë‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        else:
            grade = "D"
            comment = "ì„±ëŠ¥ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤."
        
        print(f"ğŸ“ˆ ì„±ëŠ¥ ë“±ê¸‰: {grade}")
        print(f"ğŸ’¬ {comment}")
        
        # ìƒì„¸ ë¶„ì„
        self._detailed_analysis()
        
        return {
            'overall_accuracy': overall_accuracy,
            'metric_averages': metric_averages,
            'performance_stats': {
                'avg_analysis_time': statistics.mean(analysis_times),
                'avg_confidence': statistics.mean(confidences)
            },
            'priority_performance': priority_scores,
            'grade': grade,
            'detailed_results': self.results
        }
    
    def _detailed_analysis(self):
        """ìƒì„¸ ë¶„ì„ ë° ê°œì„  ì œì•ˆ"""
        print(f"\nğŸ” ìƒì„¸ ë¶„ì„ ë° ê°œì„  ì œì•ˆ:")
        
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ ë©”íŠ¸ë¦­ ì°¾ê¸°
        all_scores = {}
        for result in self.results:
            for metric, score in result['scores'].items():
                if metric not in all_scores:
                    all_scores[metric] = []
                all_scores[metric].append(score)
        
        metric_averages = {metric: statistics.mean(scores) for metric, scores in all_scores.items()}
        worst_metric = min(metric_averages.keys(), key=lambda k: metric_averages[k])
        
        print(f"   âš ï¸ ê°œì„  í•„ìš” ì˜ì—­: {worst_metric} ({metric_averages[worst_metric]:.1%})")
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
        failed_cases = [r for r in self.results if sum(r['scores'].values()) / len(r['scores']) < 0.7]
        if failed_cases:
            print(f"   âŒ ì €ì„±ëŠ¥ ì¼€ì´ìŠ¤ ({len(failed_cases)}ê°œ):")
            for case in failed_cases:
                print(f"      â€¢ {case['test_id']}: {sum(case['scores'].values()) / len(case['scores']):.1%}")
    
    def save_results(self, filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            filename = f"nlp_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    
    def generate_csv_report(self, filename: str = None):
        """CSV í˜•íƒœì˜ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        if filename is None:
            filename = f"nlp_evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # ë°ì´í„° ì¤€ë¹„
        data = []
        for result in self.results:
            row = {
                'test_id': result['test_id'],
                'text': result['text'],
                'analysis_time': result['analysis_time'],
                'confidence': result['confidence'],
                'priority': result['priority'],
                'overall_score': sum(result['scores'].values()) / len(result['scores'])
            }
            row.update(result['scores'])
            data.append(row)
        
        # CSV ì €ì¥
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“Š CSV ë³´ê³ ì„œê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¥‹ BJJ V2 NLP ì„±ëŠ¥ í‰ê°€ê¸° ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    try:
        # í‰ê°€ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        evaluator = BJJNLPEvaluator()
        summary = evaluator.run_full_evaluation()
        
        # ê²°ê³¼ ì €ì¥
        json_file = evaluator.save_results()
        csv_file = evaluator.generate_csv_report()
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“‹ JSON ìƒì„¸ ê²°ê³¼: {json_file}")
        print(f"ğŸ“Š CSV ìš”ì•½ ë³´ê³ ì„œ: {csv_file}")
        
        return summary
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()

    
    
# ì‹¤í–‰ì½”ë“œ py bjj_nlp_evaluator.py